{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# minimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    49 loss: 1.95360065\n",
      "epoch:    99 loss: 1.89844728\n",
      "epoch:   149 loss: 1.88555324\n",
      "epoch:   199 loss: 1.46310210\n",
      "epoch:   249 loss: 1.20909953\n",
      "epoch:   299 loss: 0.65845472\n",
      "epoch:   349 loss: 1.01334047\n",
      "epoch:   399 loss: 0.88463944\n",
      "epoch:   449 loss: 0.50584602\n",
      "epoch:   499 loss: 0.41929311\n",
      "epoch:   549 loss: 0.34425122\n",
      "epoch:   599 loss: 0.34975883\n",
      "epoch:   649 loss: 0.29601127\n",
      "epoch:   699 loss: 0.25800046\n",
      "epoch:   749 loss: 0.18110007\n",
      "epoch:   799 loss: 0.17918137\n",
      "epoch:   849 loss: 0.15019381\n",
      "epoch:   899 loss: 0.21093020\n",
      "epoch:   949 loss: 0.21180575\n",
      "epoch:   999 loss: 0.14081015\n",
      "epoch:  1049 loss: 0.16018997\n",
      "epoch:  1099 loss: 0.14671925\n",
      "epoch:  1149 loss: 0.14184570\n",
      "epoch:  1199 loss: 0.13095742\n",
      "epoch:  1249 loss: 0.14048429\n",
      "epoch:  1299 loss: 0.09658053\n",
      "epoch:  1349 loss: 0.09333814\n",
      "epoch:  1399 loss: 0.08692633\n",
      "epoch:  1449 loss: 0.09685785\n",
      "epoch:  1499 loss: 0.07843018\n",
      "epoch:  1549 loss: 0.06664760\n",
      "epoch:  1599 loss: 0.07026114\n",
      "epoch:  1649 loss: 0.06158974\n",
      "epoch:  1699 loss: 0.06588688\n",
      "epoch:  1749 loss: 0.05537986\n",
      "epoch:  1799 loss: 0.04778701\n",
      "epoch:  1849 loss: 0.04335054\n",
      "epoch:  1899 loss: 0.04294939\n",
      "epoch:  1949 loss: 0.03868752\n",
      "epoch:  1999 loss: 0.03287434\n",
      "epoch:  2049 loss: 0.03235662\n",
      "epoch:  2099 loss: 0.02450345\n",
      "epoch:  2149 loss: 0.02405567\n",
      "epoch:  2199 loss: 0.02258620\n",
      "epoch:  2249 loss: 0.02043020\n",
      "epoch:  2299 loss: 0.01990554\n",
      "epoch:  2349 loss: 0.01490338\n",
      "epoch:  2399 loss: 0.01551671\n",
      "epoch:  2449 loss: 0.01100369\n",
      "epoch:  2499 loss: 0.01383427\n",
      "epoch:  2549 loss: 0.01221409\n",
      "epoch:  2599 loss: 0.00953632\n",
      "epoch:  2649 loss: 0.00900926\n",
      "epoch:  2699 loss: 0.00851335\n",
      "epoch:  2749 loss: 0.00811282\n",
      "epoch:  2799 loss: 0.00717484\n",
      "epoch:  2849 loss: 0.00700516\n",
      "epoch:  2899 loss: 0.00556709\n",
      "epoch:  2949 loss: 0.00577179\n",
      "epoch:  2999 loss: 0.00499933\n",
      "epoch:  3049 loss: 0.00425504\n",
      "epoch:  3099 loss: 0.00407989\n",
      "epoch:  3149 loss: 0.00371454\n",
      "epoch:  3199 loss: 0.00332486\n",
      "epoch:  3249 loss: 0.00335543\n",
      "epoch:  3299 loss: 0.00333907\n",
      "epoch:  3349 loss: 0.00287606\n",
      "epoch:  3399 loss: 0.00342145\n",
      "epoch:  3449 loss: 0.00287953\n",
      "epoch:  3499 loss: 0.00233890\n",
      "epoch:  3549 loss: 0.00233747\n",
      "epoch:  3599 loss: 0.00203086\n",
      "epoch:  3649 loss: 0.00202720\n",
      "epoch:  3699 loss: 0.00169517\n",
      "epoch:  3749 loss: 0.00171180\n",
      "epoch:  3799 loss: 0.00179611\n",
      "epoch:  3849 loss: 0.00153260\n",
      "epoch:  3899 loss: 0.00110482\n",
      "epoch:  3949 loss: 0.00109799\n",
      "epoch:  3999 loss: 0.00130803\n",
      "epoch:  4049 loss: 0.00101315\n",
      "epoch:  4099 loss: 0.00100347\n",
      "epoch:  4149 loss: 0.00084770\n",
      "epoch:  4199 loss: 0.00076934\n",
      "epoch:  4249 loss: 0.00088063\n",
      "epoch:  4299 loss: 0.00062693\n",
      "epoch:  4349 loss: 0.00060796\n",
      "epoch:  4399 loss: 0.00052848\n",
      "epoch:  4449 loss: 0.00050645\n",
      "epoch:  4499 loss: 0.00040749\n",
      "epoch:  4549 loss: 0.00036592\n",
      "epoch:  4599 loss: 0.00033920\n",
      "epoch:  4649 loss: 0.00033697\n",
      "epoch:  4699 loss: 0.00026394\n",
      "epoch:  4749 loss: 0.00044856\n",
      "epoch:  4799 loss: 0.00059622\n",
      "epoch:  4849 loss: 0.00025943\n",
      "epoch:  4899 loss: 0.00019832\n",
      "epoch:  4949 loss: 0.00014874\n",
      "epoch:  4999 loss: 0.00011437\n",
      "epoch:  5049 loss: 0.00056869\n",
      "epoch:  5099 loss: 0.00033649\n",
      "epoch:  5149 loss: 0.00016192\n",
      "epoch:  5199 loss: 0.00009758\n",
      "epoch:  5249 loss: 0.00008475\n",
      "epoch:  5299 loss: 0.00006061\n",
      "epoch:  5349 loss: 0.00006244\n",
      "epoch:  5399 loss: 0.00004958\n",
      "epoch:  5449 loss: 0.00004128\n",
      "epoch:  5499 loss: 0.00003079\n",
      "epoch:  5549 loss: 0.00002576\n",
      "epoch:  5599 loss: 0.00004038\n",
      "epoch:  5649 loss: 0.00003314\n",
      "epoch:  5699 loss: 0.00008369\n",
      "epoch:  5749 loss: 0.00028292\n",
      "epoch:  5799 loss: 0.00026383\n",
      "epoch:  5849 loss: 0.00010206\n",
      "epoch:  5899 loss: 0.00002953\n",
      "epoch:  5949 loss: 0.00003390\n",
      "epoch:  5999 loss: 0.00002255\n",
      "epoch:  6049 loss: 0.00000998\n",
      "epoch:  6099 loss: 0.00000942\n",
      "epoch:  6149 loss: 0.00001004\n",
      "epoch:  6199 loss: 0.00001132\n",
      "epoch:  6249 loss: 0.00000969\n",
      "epoch:  6299 loss: 0.00000640\n",
      "epoch:  6349 loss: 0.00006570\n",
      "epoch:  6399 loss: 0.00001918\n",
      "epoch:  6449 loss: 0.00061976\n",
      "epoch:  6499 loss: 0.03447161\n",
      "epoch:  6549 loss: 0.00677331\n",
      "epoch:  6599 loss: 0.00147522\n",
      "epoch:  6649 loss: 0.00037430\n",
      "epoch:  6699 loss: 0.00024864\n",
      "epoch:  6749 loss: 0.00014261\n",
      "epoch:  6799 loss: 0.00009157\n",
      "epoch:  6849 loss: 0.00007119\n",
      "epoch:  6899 loss: 0.00008172\n",
      "epoch:  6949 loss: 0.00006052\n",
      "epoch:  6999 loss: 0.00003088\n",
      "epoch:  7049 loss: 0.00002639\n",
      "epoch:  7099 loss: 0.00002146\n",
      "epoch:  7149 loss: 0.00001734\n",
      "epoch:  7199 loss: 0.00001638\n",
      "epoch:  7249 loss: 0.00001908\n",
      "epoch:  7299 loss: 0.00001634\n",
      "epoch:  7349 loss: 0.00001049\n",
      "epoch:  7399 loss: 0.00000671\n",
      "epoch:  7449 loss: 0.00002463\n",
      "epoch:  7499 loss: 0.00001725\n",
      "epoch:  7549 loss: 0.00000504\n",
      "epoch:  7599 loss: 0.00000330\n",
      "epoch:  7649 loss: 0.00000319\n",
      "epoch:  7699 loss: 0.00000333\n",
      "epoch:  7749 loss: 0.00000178\n",
      "epoch:  7799 loss: 0.00000373\n",
      "epoch:  7849 loss: 0.00027769\n",
      "epoch:  7899 loss: 0.00157333\n",
      "epoch:  7949 loss: 0.00033608\n",
      "epoch:  7999 loss: 0.00003434\n",
      "epoch:  8049 loss: 0.00001408\n",
      "epoch:  8099 loss: 0.00001767\n",
      "epoch:  8149 loss: 0.00000434\n",
      "epoch:  8199 loss: 0.00000258\n",
      "epoch:  8249 loss: 0.00000116\n",
      "epoch:  8299 loss: 0.00000126\n",
      "epoch:  8349 loss: 0.00000080\n",
      "epoch:  8399 loss: 0.00000081\n",
      "epoch:  8449 loss: 0.00000049\n",
      "epoch:  8499 loss: 0.00000047\n",
      "epoch:  8549 loss: 0.00000040\n",
      "epoch:  8599 loss: 0.00000022\n",
      "epoch:  8649 loss: 0.00000020\n",
      "epoch:  8699 loss: 0.00000023\n",
      "epoch:  8749 loss: 0.00000019\n",
      "epoch:  8799 loss: 0.00000028\n",
      "epoch:  8849 loss: 0.00000030\n",
      "epoch:  8899 loss: 0.00000120\n",
      "epoch:  8949 loss: 0.00000140\n",
      "epoch:  8999 loss: 0.00000092\n",
      "epoch:  9049 loss: 0.00000060\n",
      "epoch:  9099 loss: 0.00000497\n",
      "epoch:  9149 loss: 0.00001536\n",
      "epoch:  9199 loss: 0.00002660\n",
      "epoch:  9249 loss: 0.00004071\n",
      "epoch:  9299 loss: 0.00033163\n",
      "epoch:  9349 loss: 0.00426796\n",
      "epoch:  9399 loss: 0.00320366\n",
      "epoch:  9449 loss: 0.00744804\n",
      "epoch:  9499 loss: 0.00161693\n",
      "epoch:  9549 loss: 0.00034624\n",
      "epoch:  9599 loss: 0.00003654\n",
      "epoch:  9649 loss: 0.00000959\n",
      "epoch:  9699 loss: 0.00000606\n",
      "epoch:  9749 loss: 0.00000544\n",
      "epoch:  9799 loss: 0.00000396\n",
      "epoch:  9849 loss: 0.00000293\n",
      "epoch:  9899 loss: 0.00000217\n",
      "epoch:  9949 loss: 0.00000196\n",
      "epoch:  9999 loss: 0.00000159\n"
     ]
    }
   ],
   "source": [
    "# solve after 40s of training 5000/128\n",
    "# default_maze = torch.tensor([\n",
    "#     [1, 0, 0, 0, 0],\n",
    "#     [1, 1, 1, 1, 1],\n",
    "#     [0, 1, 0, 1, 0],\n",
    "#     [1, 1, 0, 0, 0],\n",
    "#     [1, 1, 1, 1, -1],\n",
    "# ])\n",
    "\n",
    "default_maze = torch.tensor([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [1, 1, 1, 1, -1],\n",
    "])\n",
    "\n",
    "# default_maze = torch.tensor([\n",
    "#     [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#     [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#     [0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
    "#     [0, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "#     [0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "#     [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
    "#     [0, 1, 1, 1, 1, 1, 1, 0, 0, 1],\n",
    "#     [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "#     [0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "#     [0, 0, 0, 0, 1, 1, 1, 1, 1, -1],\n",
    "# ])\n",
    "\n",
    "# doesn't solve even after 2h of training 100000/512\n",
    "# default_maze = torch.tensor([\n",
    "#     [1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1],\n",
    "#     [1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1],\n",
    "#     [1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0],\n",
    "#     [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
    "#     [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
    "#     [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "#     [1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0],\n",
    "#     [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0],\n",
    "#     [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
    "#     [0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
    "#     [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n",
    "#     [1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "#     [1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "#     [1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "#     [1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0],\n",
    "#     [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1],\n",
    "#     [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, -1],\n",
    "# ])\n",
    "MAZE_WIDTH = default_maze.shape[0]\n",
    "INPUT_SIZE = MAZE_WIDTH * MAZE_WIDTH + 2 * MAZE_WIDTH\n",
    "MOVES = {\n",
    "    (-1, 0): torch.tensor(0).to(device), # up\n",
    "    (1, 0):  torch.tensor(1).to(device), # down\n",
    "    (0, -1): torch.tensor(2).to(device), # left\n",
    "    (0, 1):  torch.tensor(3).to(device),  # right\n",
    " }\n",
    "\n",
    "# policy\n",
    "HIT_WALL_PENALTY = -1\n",
    "MOVE_PENALTY = 0\n",
    "WIN_REWARD = 10\n",
    " \n",
    "# hyperparams\n",
    "# BATCH_SIZE = 128\n",
    "EPOCH = 10000\n",
    "BATCH_SIZE = 512\n",
    "# EPOCH = 100000\n",
    "# EPOCH = 5000\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "def get_maze():\n",
    "    maze = default_maze\n",
    "    rewards = torch.zeros_like(maze)\n",
    "    rewards[maze == 0] = HIT_WALL_PENALTY\n",
    "    rewards[maze == 1] = MOVE_PENALTY\n",
    "    rewards[maze == -1] = WIN_REWARD\n",
    "    return maze, rewards\n",
    "\n",
    "def get_reward(rewards, pos):\n",
    "    x, y = pos\n",
    "    a, b = rewards.shape\n",
    "    if 0 <= x < a and 0 <= y < b:\n",
    "        return rewards[x, y]\n",
    "    return HIT_WALL_PENALTY\n",
    "\n",
    "def get_next_pos(maze, rewards, pos):\n",
    "    new_pos = pos # default to bouncing off a wall.\n",
    "    reward = HIT_WALL_PENALTY # default to hitting a wall.\n",
    "    move = random.choice(list(MOVES.keys()))\n",
    "    x, y = pos\n",
    "    a, b = maze.shape\n",
    "    i, j = move\n",
    "    if 0 <= x + i < a and 0 <= y + j < b:\n",
    "        # if maze[x + i, y + j] != -1:\n",
    "        new_pos = (x + i, y + j)\n",
    "        reward = get_reward(rewards, new_pos)\n",
    "    return new_pos, reward, move\n",
    "\n",
    "# def get_batch():\n",
    "#     batch = []\n",
    "#     maze, rewards = get_maze()\n",
    "#     positions = random.choices((maze == 1).nonzero().tolist(), k=BATCH_SIZE)\n",
    "#     for pos in positions:\n",
    "#         new_pos, reward, move = get_next_pos(maze, rewards, pos)\n",
    "#         batch.append((pos, move, new_pos, reward))\n",
    "#     return maze, batch\n",
    "\n",
    "def get_batch():\n",
    "    batch = []\n",
    "    maze, rewards = get_maze()\n",
    "    for pos in (maze == 1).nonzero().tolist():\n",
    "    # for pos in positions:\n",
    "        new_pos, reward, move = get_next_pos(maze, rewards, pos)\n",
    "        batch.append((pos, move, new_pos, reward))\n",
    "    return maze, batch\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(INPUT_SIZE, INPUT_SIZE),\n",
    "            nn.LayerNorm(INPUT_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(INPUT_SIZE, INPUT_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(INPUT_SIZE, INPUT_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(INPUT_SIZE, len(MOVES)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)        \n",
    "        return logits\n",
    "\n",
    "def to_input(maze, pos):\n",
    "    return torch.cat((\n",
    "        maze.view(-1),\n",
    "        F.one_hot(torch.tensor(pos), num_classes=MAZE_WIDTH).view(-1),\n",
    "    )).float().to(device)\n",
    "\n",
    "def train(model):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    losses = []\n",
    "    for epoch in range(EPOCH):\n",
    "        maze, batch = get_batch()\n",
    "\n",
    "        # train vectorized\n",
    "        # ----------------\n",
    "        xs, ms, ys, rs, nuke = [], [], [], [], []\n",
    "        for pos, move, new_pos, reward in batch:\n",
    "            xs.append(to_input(maze, pos))\n",
    "            ms.append(F.one_hot(MOVES[move], num_classes=len(MOVES)))\n",
    "            ys.append(to_input(maze, new_pos))\n",
    "            rs.append(reward)\n",
    "            nuke.append(0. if reward == -1 else 1.)\n",
    "\n",
    "        XS = torch.stack(xs).to(device)\n",
    "        MS = torch.stack(ms).to(device)\n",
    "        YS = torch.stack(ys).to(device)\n",
    "        RS = torch.tensor(rs).to(device).view(-1, 1)\n",
    "        NUKE = torch.tensor(nuke).to(device).view(-1, 1)\n",
    "        bellman_left = (model(XS) * MS).sum(dim=1, keepdim=True)\n",
    "        qqs = model(YS).max(dim=1, keepdim=True).values\n",
    "\n",
    "        decay = 0.9\n",
    "        bellman_right = RS + qqs * NUKE * decay\n",
    "\n",
    "        # Bellman equation\n",
    "        # Q(m, p) = r + max(Q(m', p'))\n",
    "        loss = F.mse_loss(bellman_left, bellman_right)\n",
    "        losses.append(loss.item())\n",
    "        if epoch % 50 == 49:\n",
    "            print(f\"epoch: {epoch: 5} loss: {torch.tensor(losses).mean():.8f}\")\n",
    "            losses = []\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # train non-vectorized\n",
    "        # --------------------\n",
    "        # lefts, rights = [], []\n",
    "        # for pos, move, new_pos, reward in batch:\n",
    "        #     qs = model(to_input(maze, pos))\n",
    "        #     hot = F.one_hot(MOVES[move], num_classes=len(MOVES))\n",
    "        #     bellman_left = (qs * hot).sum()\n",
    "\n",
    "        #     new_q = model(to_input(maze, new_pos)).max()\n",
    "        #     bellman_right = reward + new_q\n",
    "\n",
    "        #     lefts.append(bellman_left)\n",
    "        #     rights.append(bellman_right)\n",
    "        \n",
    "        # bellman_left = torch.stack(lefts).to(device)\n",
    "        # bellman_right = torch.stack(rights).to(device)\n",
    "        # loss = F.mse_loss(bellman_left, bellman_right)\n",
    "        # losses.append(loss.item())\n",
    "        # if epoch % 50 == 0:\n",
    "        #     print(f\"epoch: {epoch: 5} loss: {torch.tensor(losses).mean():.8f}\")\n",
    "        #     losses = []\n",
    "        # optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  1,  1,  1,  1],\n",
      "        [ 0,  0,  0,  0,  1],\n",
      "        [ 1,  1,  1,  1,  1],\n",
      "        [ 1,  0,  0,  0,  0],\n",
      "        [ 1,  1,  1,  1, -1]])\n",
      "qs=tensor([-0.9993, -1.0012, -0.9986,  0.0546], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "chose (0, 1) from (0, 0) to (0, 1)\n",
      "qs=tensor([-0.9982, -0.9997,  0.0492,  0.0627], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "chose (0, 1) from (0, 1) to (0, 2)\n",
      "qs=tensor([-0.9999, -0.9990,  0.0571,  0.0711], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "chose (0, 1) from (0, 2) to (0, 3)\n",
      "qs=tensor([-0.9990, -1.0003,  0.0651,  0.0800], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "chose (0, 1) from (0, 3) to (0, 4)\n",
      "qs=tensor([-0.9981,  0.0892,  0.0725, -0.9993], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "chose (1, 0) from (0, 4) to (1, 4)\n",
      "qs=tensor([ 0.0819,  0.0997, -1.0012, -1.0003], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "chose (1, 0) from (1, 4) to (2, 4)\n",
      "qs=tensor([ 0.0947, -0.9999,  0.1173, -0.9979], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "chose (0, -1) from (2, 4) to (2, 3)\n",
      "qs=tensor([-0.9972, -0.9993,  0.1321,  0.1038], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "chose (0, -1) from (2, 3) to (2, 2)\n",
      "qs=tensor([-0.9981, -1.0002,  0.1470,  0.1181], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "chose (0, -1) from (2, 2) to (2, 1)\n",
      "qs=tensor([-0.9981, -0.9996,  0.1648,  0.1314], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "chose (0, -1) from (2, 1) to (2, 0)\n",
      "qs=tensor([-0.9995,  0.1825, -0.9984,  0.1468], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "chose (1, 0) from (2, 0) to (3, 0)\n",
      "qs=tensor([ 0.1644,  0.2010, -1.0005, -1.0007], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "chose (1, 0) from (3, 0) to (4, 0)\n",
      "qs=tensor([ 0.1804, -1.0007, -1.0002,  0.2239], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "chose (0, 1) from (4, 0) to (4, 1)\n",
      "qs=tensor([-1.0009, -1.0015,  0.2025,  0.2492], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "chose (0, 1) from (4, 1) to (4, 2)\n",
      "qs=tensor([-1.0016, -1.0025,  0.2248,  0.2762], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "chose (0, 1) from (4, 2) to (4, 3)\n",
      "qs=tensor([-1.0024, -1.0012,  0.2491,  0.3077], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "chose (0, 1) from (4, 3) to (4, 4)\n",
      "WIN\n",
      "qs=tensor([-1.0024, -1.0012,  0.2491,  0.3077], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "i2move = {i.detach().item(): v for v, i in MOVES.items()}\n",
    "\n",
    "def play(model, maze, pos=(0, 0)):\n",
    "    print(maze)\n",
    "    depth = 1000\n",
    "    while True:\n",
    "        qs = model(to_input(maze, pos))\n",
    "        print(f'{qs=}')\n",
    "        move = i2move[qs.argmax().tolist()]\n",
    "        new_pos = (pos[0] + move[0], pos[1] + move[1])\n",
    "        print(f'chose {move} from {pos} to {new_pos}')\n",
    "        if 0 <= new_pos[0] < MAZE_WIDTH and 0 <= new_pos[1] < MAZE_WIDTH:\n",
    "            pos = new_pos\n",
    "            if maze[pos] == -1:\n",
    "                print(\"WIN\")\n",
    "                break\n",
    "            elif maze[pos] == 0:\n",
    "                print(\"LOSE: HIT WALL\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"LOSE: OUTSIDE MAZE\")\n",
    "            break\n",
    "        depth -= 1\n",
    "        if depth == 0:\n",
    "            print(\"LOSE: TOO DEEP\")\n",
    "            break\n",
    "\n",
    "play(model, default_maze, pos=(0, 0))\n",
    "\n",
    "# print(default_maze)\n",
    "# qs = model(to_input(default_maze, (4, 3)))\n",
    "# print(f'{qs=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([4, 3], (0, -1), (4, 2), tensor(0))]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in get_batch()[1] if x[0] == [4, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    49 loss: 0.00001143\n",
      "epoch:    99 loss: 0.00000130\n",
      "epoch:   149 loss: 0.00000149\n",
      "epoch:   199 loss: 0.00000105\n",
      "epoch:   249 loss: 0.00000118\n",
      "epoch:   299 loss: 0.00000364\n",
      "epoch:   349 loss: 0.00000395\n",
      "epoch:   399 loss: 0.00002352\n",
      "epoch:   449 loss: 0.00000635\n",
      "epoch:   499 loss: 0.00000311\n",
      "epoch:   549 loss: 0.00000076\n",
      "epoch:   599 loss: 0.00000031\n",
      "epoch:   649 loss: 0.00000027\n",
      "epoch:   699 loss: 0.00000153\n",
      "epoch:   749 loss: 0.00000111\n",
      "epoch:   799 loss: 0.00000043\n",
      "epoch:   849 loss: 0.00000029\n",
      "epoch:   899 loss: 0.00000078\n",
      "epoch:   949 loss: 0.00000059\n",
      "epoch:   999 loss: 0.00000046\n"
     ]
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for NeuralNetwork:\n\tMissing key(s) in state_dict: \"linear_relu_stack.2.weight\", \"linear_relu_stack.2.bias\", \"linear_relu_stack.4.weight\", \"linear_relu_stack.4.bias\", \"linear_relu_stack.6.weight\", \"linear_relu_stack.6.bias\". \n\tUnexpected key(s) in state_dict: \"linear_relu_stack.7.weight\", \"linear_relu_stack.7.bias\", \"linear_relu_stack.1.weight\", \"linear_relu_stack.1.bias\", \"linear_relu_stack.3.weight\", \"linear_relu_stack.3.bias\", \"linear_relu_stack.5.weight\", \"linear_relu_stack.5.bias\". \n\tsize mismatch for linear_relu_stack.0.weight: copying a param with shape torch.Size([323, 323]) from checkpoint, the shape in current model is torch.Size([35, 35]).\n\tsize mismatch for linear_relu_stack.0.bias: copying a param with shape torch.Size([323]) from checkpoint, the shape in current model is torch.Size([35]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\p\\Desktop\\_ML\\rl\\q-learning_maze.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/p/Desktop/_ML/rl/q-learning_maze.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# backup to disk\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/p/Desktop/_ML/rl/q-learning_maze.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# torch.save(model.state_dict(), 'default-maze.pt')\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/p/Desktop/_ML/rl/q-learning_maze.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m m \u001b[39m=\u001b[39m NeuralNetwork()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/p/Desktop/_ML/rl/q-learning_maze.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m m\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mbigger-maze.pt\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for NeuralNetwork:\n\tMissing key(s) in state_dict: \"linear_relu_stack.2.weight\", \"linear_relu_stack.2.bias\", \"linear_relu_stack.4.weight\", \"linear_relu_stack.4.bias\", \"linear_relu_stack.6.weight\", \"linear_relu_stack.6.bias\". \n\tUnexpected key(s) in state_dict: \"linear_relu_stack.7.weight\", \"linear_relu_stack.7.bias\", \"linear_relu_stack.1.weight\", \"linear_relu_stack.1.bias\", \"linear_relu_stack.3.weight\", \"linear_relu_stack.3.bias\", \"linear_relu_stack.5.weight\", \"linear_relu_stack.5.bias\". \n\tsize mismatch for linear_relu_stack.0.weight: copying a param with shape torch.Size([323, 323]) from checkpoint, the shape in current model is torch.Size([35, 35]).\n\tsize mismatch for linear_relu_stack.0.bias: copying a param with shape torch.Size([323]) from checkpoint, the shape in current model is torch.Size([35])."
     ]
    }
   ],
   "source": [
    "# backup to disk\n",
    "# torch.save(model.state_dict(), 'default-maze.pt')\n",
    "\n",
    "# m = NeuralNetwork()\n",
    "# m.load_state_dict(torch.load('bigger-maze.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_hf_nlp",
   "language": "python",
   "name": "venv_hf_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
