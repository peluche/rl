{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U gymnasium\n",
    "# %pip install -U gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 3e-4\n",
    "GAMMA = 0.9\n",
    "MAX_STEPS = 1000\n",
    "N_HIDDEN = 32\n",
    "EPS = torch.finfo(torch.float32).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, n_in, n_out, n_hidden=N_HIDDEN):\n",
    "        super().__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(n_in, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_out),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy(n_in=4, n_out=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 140.72378540039062\n",
      "  100 389.76593017578125\n",
      "  200 562.3916015625\n",
      "  300 1522.986083984375\n",
      "  400 1174.462890625\n",
      "  500 1487.4931640625\n",
      "  600 782.4196166992188\n",
      "  700 301.91668701171875\n",
      "  800 1335.84375\n",
      "  900 1174.96826171875\n"
     ]
    }
   ],
   "source": [
    "def discount_rewards(rewards, gamma):\n",
    "    '''\n",
    "    Compute the discounted rewards backwards through time, e.g.\n",
    "      r0 + gamma * (r1 + gamma * (r2 + gamma * (...)))\n",
    "      [----------------------------------------------]\n",
    "         ^         [---------------------------------]\n",
    "         |           ^           [-------------------]\n",
    "      discount[0]    |             ^           [-----]\n",
    "                  discount[1]      |             ^\n",
    "                                discount[2]      |\n",
    "                                              discount[n]                                              \n",
    "    '''\n",
    "    reward = 0\n",
    "    discounted_rewards = []\n",
    "    for r in rewards[::-1]:\n",
    "        reward = r + gamma * reward\n",
    "        discounted_rewards.append(reward)\n",
    "    return torch.tensor(discounted_rewards[::-1]).to(device)\n",
    "\n",
    "def train(model, epochs=10, lr=LR, gamma=GAMMA, max_steps=MAX_STEPS, log_every=100):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    opt = optim.Adam(policy.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        rewards, log_probs = [], []\n",
    "        state, _ = env.reset()\n",
    "        # forward pass\n",
    "        for _ in range(MAX_STEPS):\n",
    "            # this feel bad performance wise, 1 cycle GPU, 1 cycle CPU\n",
    "            # it's too much context switching\n",
    "            # how can I benchmark this in a notebook?\n",
    "            actions = policy(torch.tensor(state).float().to(device))\n",
    "            m = torch.distributions.Categorical(actions)\n",
    "            action = m.sample()\n",
    "            state, reward, done, _, _ = env.step(action.item())\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(m.log_prob(action))\n",
    "            if done:\n",
    "                break\n",
    "        # backward pass\n",
    "        losses = []\n",
    "        discounted_rewards = discount_rewards(rewards, gamma)\n",
    "        for log_prob, reward in zip(log_probs, discounted_rewards):\n",
    "            losses.append(-log_prob * reward)\n",
    "        loss = torch.stack(losses).sum()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if epoch % log_every == 0:\n",
    "            print(f'{epoch:5} {loss.item()}')\n",
    "    model.eval()\n",
    "\n",
    "train(policy, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "# ----\n",
    "# torch.save(policy.state_dict(), 'weights/cartpole-reinforce.pt')\n",
    "\n",
    "# load\n",
    "# ----\n",
    "# m = Policy(n_in=4, n_out=2).to(device)\n",
    "# m.load_state_dict(torch.load('weights/cartpole-reinforce.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 413 timesteps\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def show_play(env, policy, max_steps=MAX_STEPS):\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    ax.axis('off')\n",
    "    state, _ = env.reset()\n",
    "    img = ax.imshow(env.render())\n",
    "    for t in range(max_steps):\n",
    "        actions = policy(torch.tensor(state).float().to(device))\n",
    "        action = actions.argmax()\n",
    "        state, _, done, _, _ = env.step(action.item())\n",
    "        img.set_data(env.render())\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t + 1))\n",
    "            break\n",
    "\n",
    "show_play(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'state' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose()  \u001b[38;5;66;03m# Prevents duplicate display\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m HTML(anim\u001b[38;5;241m.\u001b[39mto_jshtml())  \u001b[38;5;66;03m# Display as HTML\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m display(\u001b[43mshow_play\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCartPole-v1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[83], line 24\u001b[0m, in \u001b[0;36mshow_play\u001b[1;34m(env, policy, max_steps)\u001b[0m\n\u001b[0;32m     22\u001b[0m anim \u001b[38;5;241m=\u001b[39m FuncAnimation(fig, update, frames\u001b[38;5;241m=\u001b[39mmax_steps, repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose()  \u001b[38;5;66;03m# Prevents duplicate display\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m HTML(\u001b[43manim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_jshtml\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\matplotlib\\animation.py:1352\u001b[0m, in \u001b[0;36mAnimation.to_jshtml\u001b[1;34m(self, fps, embed_frames, default_mode)\u001b[0m\n\u001b[0;32m   1348\u001b[0m         path \u001b[38;5;241m=\u001b[39m Path(tmpdir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp.html\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1349\u001b[0m         writer \u001b[38;5;241m=\u001b[39m HTMLWriter(fps\u001b[38;5;241m=\u001b[39mfps,\n\u001b[0;32m   1350\u001b[0m                             embed_frames\u001b[38;5;241m=\u001b[39membed_frames,\n\u001b[0;32m   1351\u001b[0m                             default_mode\u001b[38;5;241m=\u001b[39mdefault_mode)\n\u001b[1;32m-> 1352\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1353\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_html_representation \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mread_text()\n\u001b[0;32m   1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_html_representation\n",
      "File \u001b[1;32mc:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\matplotlib\\animation.py:1090\u001b[0m, in \u001b[0;36mAnimation.save\u001b[1;34m(self, filename, writer, fps, dpi, codec, bitrate, extra_args, metadata, extra_anim, savefig_kwargs, progress_callback)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mpl\u001b[38;5;241m.\u001b[39mrc_context({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msavefig.bbox\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}), \\\n\u001b[0;32m   1086\u001b[0m      writer\u001b[38;5;241m.\u001b[39msaving(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fig, filename, dpi), \\\n\u001b[0;32m   1087\u001b[0m      cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fig\u001b[38;5;241m.\u001b[39mcanvas,\n\u001b[0;32m   1088\u001b[0m                        _is_saving\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, manager\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m anim \u001b[38;5;129;01min\u001b[39;00m all_anim:\n\u001b[1;32m-> 1090\u001b[0m         \u001b[43manim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_draw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Clear the initial frame\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m     frame_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# TODO: Currently only FuncAnimation has a save_count\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m#       attribute. Can we generalize this to all Animations?\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\matplotlib\\animation.py:1748\u001b[0m, in \u001b[0;36mFuncAnimation._init_draw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1740\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1741\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not start iterating the frames for the initial draw. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1742\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis can be caused by passing in a 0 length sequence \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1745\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit may be exhausted due to a previous display or save.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1746\u001b[0m         )\n\u001b[0;32m   1747\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m-> 1748\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drawn_artists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_func()\n",
      "File \u001b[1;32mc:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\matplotlib\\animation.py:1767\u001b[0m, in \u001b[0;36mFuncAnimation._draw_frame\u001b[1;34m(self, framedata)\u001b[0m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_seq[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_count:]\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# Call the func with framedata and args. If blitting is desired,\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# func needs to return a sequence of any artists that were modified.\u001b[39;00m\n\u001b[1;32m-> 1767\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drawn_artists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframedata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blit:\n\u001b[0;32m   1771\u001b[0m     err \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe animation function must return a sequence \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1772\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mof Artist objects.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[83], line 13\u001b[0m, in \u001b[0;36mshow_play.<locals>.update\u001b[1;34m(frame)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(frame):\n\u001b[1;32m---> 13\u001b[0m     actions \u001b[38;5;241m=\u001b[39m policy(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mstate\u001b[49m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     14\u001b[0m     m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(actions)\n\u001b[0;32m     15\u001b[0m     action \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39msample()\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'state' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def show_play(env, policy, max_steps=MAX_STEPS):\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    ax.axis('off')\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    def update(frame):\n",
    "        actions = policy(torch.tensor(state).float().to(device))\n",
    "        m = torch.distributions.Categorical(actions)\n",
    "        action = m.sample()\n",
    "        state, _, done, _, _ = env.step(action.item())\n",
    "        ax.imshow(env.render(mode='rgb_array'))  # Ensure using rgb_array mode\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(frame + 1))\n",
    "            anim.event_source.stop()\n",
    "    \n",
    "    anim = FuncAnimation(fig, update, frames=max_steps, repeat=False)\n",
    "    plt.close()  # Prevents duplicate display\n",
    "    return HTML(anim.to_jshtml())  # Display as HTML\n",
    "\n",
    "display(show_play(gym.make('CartPole-v1'), policy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, n_in, n_out, n_hidden=10):\n",
    "        super().__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(n_in, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_out),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.f(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden=10):\n",
    "        super().__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(n_in, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.f(x)\n",
    "\n",
    "actor = Actor(n_in=10, n_out=2)\n",
    "critic = Critic(n_in=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=100, lr=LR):\n",
    "    actor_opt = optim.Adam(actor.parameters(), lr=lr)\n",
    "    critic_opt = optim.Adam(critic.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action_probs = actor(torch.from_numpy(state).float())\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            V_current = critic(torch.from_numpy(state).float())\n",
    "            V_next = critic(torch.from_numpy(next_state).float())\n",
    "\n",
    "            # Compute advantage and TD-target\n",
    "            advantage = reward + (1 - int(done)) * V_next - V_current\n",
    "            td_target = reward + (1 - int(done)) * V_next\n",
    "\n",
    "            # Update the critic\n",
    "            critic_loss = advantage.pow(2)\n",
    "            critic_opt.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_opt.step()\n",
    "\n",
    "            # Update the actor\n",
    "            actor_loss = -torch.log(action_probs[action]) * advantage.detach()\n",
    "            actor_opt.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_opt.step()\n",
    "\n",
    "            state = next_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_hf_nlp",
   "language": "python",
   "name": "venv_hf_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
