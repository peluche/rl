{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U gymnasium\n",
    "# %pip install -U gymnasium[classic-control]\n",
    "# %pip install -U ipywidgets\n",
    "# %pip install -U swig\n",
    "# # note: this is required for LunarLander and for windows you'll need:\n",
    "# #       \"Visual Studio Build Tools\" with the \"Desktop development with C++\" for a whooping 6GB of disk -_-\n",
    "# %pip install -U gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 3e-4\n",
    "GAMMA = 0.99\n",
    "MAX_STEPS = 1000\n",
    "N_HIDDEN = 32\n",
    "EPS = torch.finfo(torch.float32).eps\n",
    "ENTROPY_REGULARIZATION = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "env = gym.make(\"LunarLander-v2\", render_mode='rgb_array')\n",
    "# env = gym.make(\"LunarLander-v2\", render_mode='human')\n",
    "\n",
    "IN = env.observation_space.shape[0]\n",
    "OUT = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score(scores, validations=None, window=50, validation_rate=100):\n",
    "    averages = []\n",
    "    running_sum = sum(scores[:window])\n",
    "    for i in range(window, len(scores)):\n",
    "        running_sum += scores[i] - scores[i - window]\n",
    "        averages.append(running_sum / window)\n",
    "    plt.plot(scores, label='score')\n",
    "    plt.plot(averages, label=f'windowed average ({window})')\n",
    "    if validations:\n",
    "        x = torch.arange(len(validations)) * validation_rate\n",
    "        plt.plot(x, validations, label='validation')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validation(env, policy, max_steps=MAX_STEPS, epochs=5):\n",
    "    episodes_len = 0\n",
    "    rewards = 0\n",
    "    for _ in range(epochs):\n",
    "        state, _ = env.reset()\n",
    "        for t in range(max_steps):\n",
    "            action = policy(torch.tensor(state).to(device)).argmax().item()\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "        episodes_len += t\n",
    "    return episodes_len / epochs, rewards / epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def show_play(env, policy, max_steps=MAX_STEPS):\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    ax.axis('off')\n",
    "    state, _ = env.reset()\n",
    "    img = ax.imshow(env.render())\n",
    "    score = 0\n",
    "    for t in range(max_steps):\n",
    "        actions = policy(torch.tensor(state).float().to(device))\n",
    "        action = actions.argmax()\n",
    "        state, reward, done, _, _ = env.step(action.item())\n",
    "        score += reward\n",
    "        img.set_data(env.render())\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t + 1))\n",
    "            break\n",
    "    print(f'{score=:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma):\n",
    "    '''\n",
    "    Compute the discounted rewards backwards through time, e.g.\n",
    "      r0 + gamma * (r1 + gamma * (r2 + gamma * (...)))\n",
    "      [----------------------------------------------]\n",
    "         ^         [---------------------------------]\n",
    "         |           ^           [-------------------]\n",
    "      discount[0]    |             ^           [-----]\n",
    "                  discount[1]      |             ^\n",
    "                                discount[2]      |\n",
    "                                              discount[n]                                              \n",
    "    '''\n",
    "    reward = 0\n",
    "    discounted_rewards = []\n",
    "    for r in rewards[::-1]:\n",
    "        reward = r + gamma * reward\n",
    "        discounted_rewards.append(reward)\n",
    "    return torch.tensor(discounted_rewards[::-1], dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REINFORCE â†’ REward Increment = Nonnegative Factor x Offset Reinforcement x Characteristic Eligibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforcePolicy(nn.Module):\n",
    "    def __init__(self, n_in, n_out, n_hidden=N_HIDDEN):\n",
    "        super().__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(n_in, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_out),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_policy = ReinforcePolicy(n_in=IN, n_out=OUT).to(device)\n",
    "r_scores = []\n",
    "r_validations = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     loss=  -0.13 explore=70.16 % reward=-265.61 v_rew=-291.66 score=  124 validation=68\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[430], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m         scores\u001b[38;5;241m.\u001b[39mappend(t)\n\u001b[0;32m     40\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 42\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m plot_score(r_scores, r_validations)\n",
      "Cell \u001b[1;32mIn[430], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, epochs, lr, gamma, max_steps, log_every, scores, entropy_regularization)\u001b[0m\n\u001b[0;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(losses)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m+\u001b[39m entropy_regularization \u001b[38;5;241m*\u001b[39m entropy_loss\n\u001b[0;32m     31\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m log_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, epochs=10, lr=LR, gamma=GAMMA, max_steps=MAX_STEPS, log_every=100, scores=r_scores, entropy_regularization=ENTROPY_REGULARIZATION):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        rewards, log_probs, entropies = [], [], []\n",
    "        explored = 0\n",
    "        state, _ = env.reset()\n",
    "        # forward\n",
    "        for t in range(max_steps):\n",
    "            # this feel bad performance wise, 1 cycle GPU, 1 cycle CPU\n",
    "            # it's too much context switching\n",
    "            # how can I benchmark this in a notebook?\n",
    "            actions = model(torch.tensor(state).to(device))\n",
    "            action = torch.multinomial(actions, 1)\n",
    "            explored += action != actions.argmax()\n",
    "            state, reward, done, _, _ = env.step(action.item())\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(actions[action].log())\n",
    "            entropies.append(-(actions * torch.log(actions)).sum()) # prey for no NaNs (otherwise add a + EPS)\n",
    "            if done:\n",
    "                break\n",
    "        # backward\n",
    "        losses = []\n",
    "        discounted_rewards = discount_rewards(rewards, gamma)\n",
    "        normalized_discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + EPS)\n",
    "        for log_prob, reward in zip(log_probs, normalized_discounted_rewards):\n",
    "            losses.append(-log_prob * reward)\n",
    "        entropy_loss = - torch.stack(entropies).sum() # negative because we want to increase entropy\n",
    "        loss = torch.stack(losses).sum() + entropy_regularization * entropy_loss\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if epoch % log_every == 0:\n",
    "            val_len, val_score = validation(env, model)\n",
    "            r_validations.append(val_len)\n",
    "            print(f'{epoch:<5} loss={loss.item():>7.2f} explore={100 * explored.item() / t:>5.2f} % reward={sum(rewards):>7.2f} v_rew={val_score:>7.2f} score={t:>5} validation={val_len:.0f}')\n",
    "\n",
    "        scores.append(t)\n",
    "    model.eval()\n",
    "\n",
    "train(r_policy, epochs=1000)\n",
    "plot_score(r_scores, r_validations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "# ----\n",
    "# torch.save(r_policy.state_dict(), 'weights/lunarlander-reinforce.pt')\n",
    "\n",
    "# load\n",
    "# ----\n",
    "# m = ReinforcePolicy(n_in=4, n_out=2).to(device)\n",
    "# m.load_state_dict(torch.load('weights/cartpole-reinforce.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 126 timesteps\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEWCAYAAACqitpwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAYp0lEQVR4nO3deWyc5aHv8d87u2fz2PEy8b4kY2dxvAXHS5yEOHFIIIladEn/4Fzao+qiqhwVVaiNxOk55fzVFqGqatHpoT0qqFyOLrdVbqE3QErKCSGE5STNwpKNLASIcezYcbzFHs+cP6Y2oQTimMceL98PQrYnszyeTOY7zzzv+44Vj8fjAgDAIFuyBwAAmH2ICwDAOOICADCOuAAAjCMuAADjiAsAwDjiAgAwjrgAAIwjLgAA4xzjPaNlWZM5DgDADDGeA7swcwEAGEdcAADGERcAgHHEBQBgHHEBABhHXAAAxhEXAIBxxAUAYBxxAQAYR1wAAMYRFwCAccQFAGAccQEAGEdcAADGERcAgHHEBQBgHHEBABhHXAAAxhEXAIBxxAUAYBxxAQAYR1wAAMYRFwCAccQFAGAccQEAGEdcAADGERcAgHHEBQBgHHEBABhHXAAAxhEXAIBxxAUAYBxxAQAYR1wAAMYRFwCAccQFAGAccQEAGEdcAADGERcAgHHEBQBgHHEBABhHXAAAxhEXAIBxxAUAYBxxAQAYR1wAAMYRFwCAccQFAGAccQEAGEdcAADGERcAgHHEBQBgHHEBABhHXAAAxhEXAIBxxAUAYBxxAQAYR1wAAMYRFwCAccQFAGAccQEAGEdcAADGERcAgHHEBQBgHHEBABhHXAAAxhEXAIBxxAUAYBxxAQAYR1wAAMYRFwCAccQFAGAccQEAGEdcAADGERcAgHHEBQBgHHEBABhHXAAAxhEXAIBxxAUAYBxxAQAYR1wAAMYRFwCAccQFAGAccQEAGEdcAADGERcAgHHEBQBgHHEBABhHXAAAxhEXAIBxxAUAYBxxAQAYR1wAAMYRFwCAccQFAGAccQEAGEdcAADGERcAgHHEBQBgHHEBABhHXAAAxhEXAIBxxAUAYBxxAQAYR1wAAMYRFwCAccQFAGAccQEAGEdcAADGERcAgHHEBQBgHHEBABhHXAAAxhEXAIBxxAUAYBxxAQAYR1wAAMYRFwCAccQFAGAccQEAGEdcAADGERcAgHHEBQBgHHEBABhHXAAAxhEXAIBxxAUAYBxxAQAYR1wAAMYRFwCAccQFAGAccQEAGEdcAADGERcAgHHEBQBgHHEBABhHXAAAxhEXAIBxxAUAYBxxAQAYR1wAAMYRFwCAccQFAGAccQEAGEdcAADGERcAgHHEBQBgHHEB5hCv3a6vl5aqKTOTf/yYVDy+gDmkKStLS1JTtSU/Xz6nM9nDwSzmSPYAAEydVy9eVIHPp5M9PeobHk72cDCLWfF4PD6uM1rWZI8FADADjCcbvC0GADCOuAAAjCMuAADjiAsAwDjiAgAwjrgAAIwjLgAA49iJEtPOP/+zVFgoXb0qPfOMdORI4vTeXuny5eSObboqKpL+6Z+keFy6dEn6139N3H+xmNTRIbG/JKYaccG0s3ChVFaW+H758sQTZiwmHT0qvfpq4uf2dmnnzuSOczpJSZEqKj75ec2axP00OJi4nz7+WIpGpTfekI4fT9owMYcQF0xL1x4QwrIkm02qrpaqqhKnDQ5K99yTeALt6ZEeeywxq4nFpLY2qa8vKcNOqr89iIZlSV6vdOediZ/jcamz85P76dAhaceOxJ9dvSq9//6UDhezHId/wbTz5JNSefnNXWb0UTw0JL30kvTQQ3PrraBFi6Tf/vbmLxePJ/7v6pIeflh68UXzY8PsM55sMHPBjDX6+B4elk6cSIRlcFDatSvxFhA+69rnhM7OxGwlHk+8bXbwYPLGhdmHuGBau/bJMBaTRkYSp330kfQf//HJusJ//qc0MJC0YU47195v0Wji5+HhxP10+HDivnz/fYKCyUNcMC3F44mQnDqVWLwfGUksRu/alfjzkZHE1mP4xGhQBgakv/wlcR/19SUi/OGHn/zZ0FDyxoi5g7hgGvq2HnzwUUWj0jvvJBbo8cXc7mLt21eunTufU3+/tH9/Ii5AshAXTEMN2rXr0WQPYkax29N17ly5du16LtlDASSxhz4AYBIQFwCAcbwtBkwCm80hl9Mrm90hm82u/v5LisVYBMHcQVwAQ3y+DKWHCuX2+BT0Z6sgXKfUQK6uDFzQiy//RD1XPk72EIEpQ1yAm2LJ6fTI7fLJbncqPa1IeeEq+Xzp8qVkKc1bqEBKjtwOv+yWSzbLqcsp7ys7s5y4YE4hLsAN2O1OhVLzlBbKk8cT0LzQAuVkVMjvzVaKMyS3PVVuR6rslvO6h0kKuOZradkWnTy9JwmjB5KDuADX4XSkqDyyXjnZS+VNSVcwJVdBT468rkw5bIkZic2yS7JueNw9hy1Fqd5czZtXrM7OMxMaj9fj0eKSEl24eFEfXrw4oesAphJxAa4j5M/T8iV3K8MXkdeZIcua+IaVlmUpO7hU+TnVunTpnOLx2E1fR25WlrwejwpzcnSho0Ox8R1vFkgaNkUGriMWH9FwdEAOm+dLhWWUy+5XUU6j3G7/hC5/5sMPdbGrS8fOnCEsmBGIC3AdsdiIoiODE5plXI/Nciovc7nmpRdP6PLRkRG9e+aMLvX0GBkPMNmIC3AdsXhUw9FBxWQmLpZlKdWdp8pFXzUyEwKmOx7lwHXEYiOKRq8am7lIiYX9tEChgsGwseuc7pxOp0KhkILBoBwOlnhvJCUlReFwWN/4xjf085//XNnZ2cke0oTxtw1cRywWVTQ6qLihmcuonFC18nKq1NNzYVyf5jdTZWdnq76+XqtWrdKWLVvU19en559/Xvv27dOBAwd04cLs/v1vhmVZWrJkiZYvX65NmzappaVFwWBQNptNVVVVuu+++3T48OFkD/OmERdAicO1ZGSU6NKlc4pGr469LWZy5mJZllx2n0oLV+nke3s0NNQnm82heemFCgQzdebMG0Zvbyo5nU6lpaVp8eLFuuuuu9TY2KiioiKlpqaOnaeyslJdXV06f/68XnvtNf3xj3/UgQMH1NXVpYE59klvPp9PoVBI69at09atW7Vs2TLl5eXJ7XZ/6nxNTU369a9/rQceeEB79+5VLDZzHh/EBZBUXNCguqr/qRNnd+v4qd2KDg0l3haT2eOBOWweZYXK5XSmyOFwqmxBi6oiX9P5j/9Lly+3qbPzrNHbm2zp6elauXKl1qxZo9bWVi1atEiW9fn7/qSlpSkUCqmiokLf/OY3dfbsWb388st66aWX9Nprr+nEiRNT/BtMrUgkoubmZrW0tKilpUWZmZmS9Ln3l2VZqq2t1ZNPPqn7779fO3bsmDGBIS6Y8wpyblHz8m8rL1Sn+cEquT1eHT7yjIYNbi02yrJs8thTFQxkKzejWkvKNmp+sErzg5VyOj16ce+PNTw8aPQ2TXI6ncrMzFRZWZm++tWvat26dQqHw0pNTb3hzqSjRs9nWZZKSkpUXFysu+66S+3t7Tpy5IieffZZ7dmzR+3t7bp8+fJk/jqTzufzKTs7Ww0NDdq2bZsqKioUDofl8XjGfR2WZSk3N1e//OUvlZubq1/96lczYqZHXDCn2WwOlRQ2ap5voeyWW5f7z+u90/s0ONgzKWsukpTiTFN6Wr6Onf6TFhau1RX3h0pPWajy3Nt1PG+Xzpx9Q/H49DqCciAQUHNzs1pbW9Xc3KyqqqovnKHcDMuy5PV6VVRUpMLCQm3evFmdnZ3at2+f9u7dq5deeklHjx7V8PCwgd9kauTn52vDhg1avXq1br31VuXk5Ej6/BnKjViWpYyMDD3yyCNKT0/XT3/602kfXuKCaSPb79fqwkKl/c2rOp/DoW+Wlclhs+nf3n1XvdGosdtsrv22yopa5XNlqX/4ot46/YzaL56U4nFdHe5TNGZ+FuFxhJQ5b6HeObZLbxx+Qk11/0teZ6YCnvlaWfUP6rnyj+roOG38dm+G3W5XTk6OFi5cqC1btmjTpk3KzMy8qRnKRIxed0ZGhrZu3aqNGzequ7tbp0+f1h/+8Aft2bNHp0+fVkdHh0am0ec4p6SkqKCgQNXV1dq2bZtuueUWpaenKyUlxejtOBwOff/731dRUZG2b9+utmn8GeDEBdNGzfz5Crrdsts+vYX8wmBQt/71ld/etja9YejYWvNCJcrILNE8b0RSXGcv7tPbJ/7/2FthI8NDGor2Grmta9ksp/wp2XK5fDr70X7lnK2UO+JXtn+Z8tNXaNGC27S/+98VjV41fttfOC6bTW63W7feeqtaW1vV2NioyspKuVyuKR3HtVwul7KyspSVlaX6+nr19PTo4MGDev311/XCCy/ozTffVH9/f1LWIWw2m8LhsDZs2KB169apvr5eJSUlk367Ho9Hd999t0KhkLZv365jx45N+m1OBHHBtPHKuXNqLCjQ0N+8In2rq0v/+9QpOW02HezoMHJbNsuuTav/RenBYjltXnX2n9Rf3n1avb2fXH9fX5f6hzoVj8e/9Kv1eDyuWHxYw7EBRWMDSvGE5PdnqLPzjN5863FlzCtRiiNNaSmlql3wd+roOqljJ3ZPydZjBQUFikQi2rx5szZt2qRwOCyv1yubbfrtBhcMBrVmzRo1NzfrW9/6ltra2vTcc89p165dOnbsmM6dOzepMxq3262SkhLV1NRo69atWrNmjQKBgNxu96TO6P6W3W7XHXfcoZKSEt1zzz06fPjwtFvoJy6YNq4MDemFU6f0d1c//Yo9Go/r/56Z2NGEr8eSTUsWbJbT5VHQnauhkV6d+Xivzpx7VdIn+150dp3W0HC/orGrctrHtwCb2HcjnvgvPqKhkV71DXeof6hDQ9Fe9fd36ULHW/qg7aD6+jolSYNDPfrL2/9HKd6gvM4MBVPyVBn5H/rwwlH19Jh/28Nut8vlcqm5uVmbNm1SQ0ODli5dKq/Xa/y2JovdblcwGFQwGNTChQt177336tixYzpw4IBefPFF7d69W93d3UbWaZxOp9LT09Xa2qrbb79dlZWVikQiSY+v3W5XRUWFnn76ad13333605/+NK0CQ1ww58zPXKr6mr9XurdUNsuhtp439ebh32pk5NNPRN1Xzis+EtfVkZ7PjctoTIZj/Roa6dXwSL+iI71StFMnPj6svv5ODQ70qr3zpNo7jmskNqx4PKZY7NPrRuc+el3zTpbIuyxd8/1VKs1aq7qqk3rljX/T4KCZ44nl5eWpoqJC69ev1+bNm5Wbmyu32530J8kvy7IseTweVVVVqbKyUnfffbe6urq0Z88e7dy5U0eOHNHx48dvagsrh8OhsrIy1dTU6Pbbb9f69evl8/nkcrmmdIYyHgsWLNATTzyhH/zgB3riiSc0NDSU7CFJIi6YgyrKt8pmd8jtCKhvuEPHz76gS5fOfeZ8ccV0uecjDc27ong8U4kZSUzxeEzR2FUNDF9S31C7+oYuKjYyoss9H+mDjw+pu/us3IrpQvdH6h/oGtcWZ3HF9NapZxTOKpc3f548jpBSUoLy++dNOC4Oh0M+n0/V1dW68847tWLFCpWXlysQCEzo+mYCy7LkdrsVDoe1bds2feUrX9H58+d19OhR7d69W88//7wuXLiggYGBz7zK93g8CgaDuu2228ZmKCUlJXI6nUn6bcYvOztbP/7xj5Wdna1HH31UXV1dyR4SccHcMzh4RX0DHbqoY+q8clKH3/l/io5cf/H8wsW3lJ9bK0mKxgZ0dfiKuq68r/auYxoaHFBfX6faLr6r7p7zisVjf10jmdhhTa4OXdF/HXlKNrelK73tev3AExoYvPnNTbOyslRXV6dVq1Zp69atKi4ult1un/EzlIlwuVwqLS1VSUmJNm/erP7+fr3++uvatWuX9u/fryNHjigvL0+1tbXatGmT1q1bp1AoJLvdPu1mKDcSCoX04IMPqrCwUN/73veSHhgrPs4D/My0Oxozj8fjGXulPZl7atttLmWmLdSiyG262HVcbx/f+bkL59kZ5aqv/Xt1d3+gj9qP6nLPRxoeHlDfQKeGhvs10ZB8kVBqngYGL+vq1Svjvozf79eyZcvU1NSkpqYmFRcXKxQKGR/bbBGLxdTW1qYPPvhAaWlpKigo+MyhV2aqWCym3/3ud9q+fbvOGFyrvNZ4skFckFTZ2dkqLy9XRUWFKioqVFtbq8WLF0/J5q+jj+kv+ieQOI+l0YhM54Mtju7UyL9VxONxvfrqq9q+fbsOHTqk3l6zm9QTF0wbNptNwWBQqampKigoUFNTkxoaGhSJRJSenq5QKJTU/SmA2SYej+vtt9/WT37yEz311FNGN9EmLkgqp9Op0tJSVVVVadmyZWOzk/z8/E89nnhsAZMjHo+rra1N3/nOd/T73//e2KbKxAVTxu12KzMzUxkZGaqoqFBDQ4Pq6+sVDocVCASm7U55wGwXj8fV09OjH/7wh3rsscfU399v5DpvhLjgpo2+rx8MBlVZWanly5eroqJCixYt0sKFCxUKhXi8ANPMwMCAHnnkEe3YsUMHDx78UtdFXGBMenq6wuGwCgsLVVNTo5UrV2rp0qUKBAJKSUlhvQSYAYaGhvTcc8/poYce0qFDhya8gQpxwU2zLEt2u10Oh0M5OTmqr6/X8uXLtXjxYhUXF6uwsHDWbLIJzEXDw8N65ZVXdM899+j8+fMTug7ignFxOp2aP3++ioqKFIlEtGLFCq1YsUJFRUVyuVxyOp1s4grMIvF4XO+8847uvfde7d+//6YX+onLHHLtPg42m+1TX693ms1mU0VFhVasWKG6ujoVFRUpPz9f8+bNk91uT/avA2AKvPfee3rooYf08ssv69y5zx4C6fMQlxnK6XTK4/HI4/HI7XaPfT/6c0pKymdO93q98vl88vv9n/p6ve/9fr+8Xq8cDsfYYUH4+wXmnng8rvb2dj3++OP60Y9+pO7u7nFf7kaIyyQYPUprMBiU3+9XIBBQIBAY+/7ar9eeZ/SJ3+12j70d5XA45HK5rvvV6XSO/c9mvgAmqru7Wz/72c/08MMPq6+v74bnJy5TzOVyqby8XA0NDWpsbNTatWs1f/58SZ9//13vdO5rAFMpHo8rFovp8ccf1y9+8QudO3fuCw98SVwmmdPpVFpamvLy8rR+/Xpt3LhRxcXFysrKksczvg+XAoDpIhqNavfu3Xr66af11FNPaXBw8LrnG082OOT+BITDYTU2NqqpqUmNjY2qqakZ+8wHIgxgpnI4HGptbVUoFFJ3d7eeffbZCX+aJzOXG7AsS6mpqcrJyVF1dbW2bt2qmpoaZWZmyu/3s9YBYNaJxWI6deqUvvvd72rnzp2fmanwttgEjS7IV1ZWqqWlRXV1daqrq1M4HE720ABgyly4cEH333+/Ojo69Oc//3nsdOJyE9xut/Lz87VgwQKtX79era2tysvLk8/nmxEfcwoAk6G3t1e/+c1vtGPHDu3Zs0exWIw1ly9iWZacTqdycnK0bt06NTc3q7q6WmVlZRwnCwD+yu/36+tf/7pCoZDa2tr07rvvjutycyoulmUpMzNTixYtUm1tre644w5VVlbK5/PJ5XLN+tkZAExEIBDQ1772Nfl8Pj3wwAPjusysj4vb7Zbf79ctt9yiDRs2aPny5SorK1NmZmayhwYAM4bT6dSWLVvG9t27kVm55uLxeLR48WItW7ZMa9eu1Zo1a5SdnS2Hw8HWXQAwBWbFzMXhcCgQCKigoEAbN27UmjVrFIlElJuby/oJACTBjI2L3W5XSUnJ2CchtrS0aOnSpWNrJzNppgUAs82MiYvX61VaWpoKCwu1du1arVq1SiUlJcrKylIgEEj28AAA15jWcfF4PKqqqlJdXZ1qa2tVV1en0tJSORyJYTM7AYDpadrExW63KxQKKSsra2whfuXKlcrIyFAoFGLtBABmkKTFZXRdJBwOq66uTg0NDaqqqlJlZaUyMzPZqgsAZrApjYvb7VZOTo5yc3O1cuVKrV69WlVVVfL5fPJ6vXy8LgDMEpMaF7vdLpfLpUgkoubmZq1YsUJLlixRJBKRz+ebzJsGACSR8bikpqaqtLRUkUhEq1ev1sqVK1VYWCi32y2n08kiPADMAV8qLjabTS6XS8FgUDU1NWppaVFVVZUWLFig3NxcjiYMAHPUTcfFZrMpLy9P5eXlqq6uVlNTk+rr65WamsrhVQAAkm4iLosWLdKqVau0du1aRSIR5efnKy0tjZgAAD5j3AeuHBkZGdt8mHUTAMAXGXdcAAAYL97TAgAYR1wAAMYRFwCAccQFAGAccQEAGEdcAADGERcAgHHEBQBgHHEBABhHXAAAxhEXAIBxxAUAYBxxAQAYR1wAAMYRFwCAccQFAGAccQEAGEdcAADGERcAgHHEBQBgHHEBABhHXAAAxhEXAIBxxAUAYBxxAQAYR1wAAMYRFwCAccQFAGAccQEAGEdcAADGERcAgHHEBQBgHHEBABhHXAAAxhEXAIBxxAUAYBxxAQAYR1wAAMYRFwCAcf8NMTLAlbuKEikAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_play(env, r_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the code in a bunch of functions to make the cProfile easier to read\n",
    "\n",
    "def gpu_world(model, arg):\n",
    "    return model(arg)\n",
    "\n",
    "def cpu_world_simulate_env(env, action):\n",
    "    return env.step(action.item())\n",
    "\n",
    "def cpu_world_get_action(actions, log_probs):\n",
    "    # Categorical seems dog slow\n",
    "    # --------------------------\n",
    "    # m = torch.distributions.Categorical(actions)\n",
    "    # action = m.sample()\n",
    "    # log_probs.append(m.log_prob(action))\n",
    "\n",
    "    # multinormal is faster\n",
    "    # ---------------------\n",
    "    action = torch.multinomial(actions, 1)\n",
    "    log_probs.append(torch.log(actions[action]))\n",
    "\n",
    "    # but they should be equivalent\n",
    "    # -----------------------------\n",
    "    # assert m.log_prob(action) - torch.log(actions[action]) < 0.001\n",
    "\n",
    "    return action\n",
    "\n",
    "def cpu_world_sample(rewards, log_probs, env, actions):\n",
    "    action = cpu_world_get_action(actions, log_probs)\n",
    "\n",
    "    state, reward, done, _, _ = cpu_world_simulate_env(env, action)\n",
    "    rewards.append(reward)\n",
    "    return state, done\n",
    "\n",
    "def cpu_world_backward_pass(rewards, log_probs, gamma):\n",
    "    losses = []\n",
    "    discounted_rewards = discount_rewards(rewards, gamma)\n",
    "    normalized_discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + EPS)\n",
    "    for log_prob, reward in zip(log_probs, normalized_discounted_rewards):\n",
    "        losses.append(-log_prob * reward)\n",
    "    return losses\n",
    "\n",
    "def gpu_world_backward_pass(losses, opt):\n",
    "    loss = torch.stack(losses).to(device).sum()\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    return loss\n",
    "\n",
    "def train_for_profiling(model, epochs=100, lr=LR, gamma=GAMMA, max_steps=MAX_STEPS):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        rewards, log_probs = [], []\n",
    "        state, _ = env.reset()\n",
    "        # forward pass\n",
    "        for _ in range(max_steps):\n",
    "            actions = gpu_world(model, torch.tensor(state).float().to(device))\n",
    "            state, done = cpu_world_sample(rewards, log_probs, env, actions)\n",
    "            if done: break\n",
    "        # backward pass\n",
    "        losses = cpu_world_backward_pass(rewards, log_probs, gamma)\n",
    "        loss = gpu_world_backward_pass(losses, opt)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'{epoch:5} {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 -3.506568670272827\n",
      "   10 -0.1585693359375\n",
      "   20 0.31032800674438477\n",
      "   30 0.8864667415618896\n",
      "   40 1.2831541299819946\n",
      "   50 0.7466640472412109\n",
      "   60 0.7476024627685547\n",
      "   70 0.15681791305541992\n",
      "   80 1.466324806213379\n",
      "   90 -2.0493550300598145\n",
      "         267342 function calls (246101 primitive calls) in 5.952 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.069    0.069    5.952    5.952 C:\\Users\\p\\AppData\\Local\\Temp\\ipykernel_33816\\2794650987.py:49(train_for_profiling)\n",
      "     4240    0.009    0.000    2.190    0.001 C:\\Users\\p\\AppData\\Local\\Temp\\ipykernel_33816\\2794650987.py:27(cpu_world_sample)\n",
      "      100    0.001    0.000    2.047    0.020 C:\\Users\\p\\AppData\\Local\\Temp\\ipykernel_33816\\2794650987.py:42(gpu_world_backward_pass)\n",
      "      100    0.001    0.000    1.971    0.020 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\_tensor.py:428(backward)\n",
      "      100    0.001    0.000    1.970    0.020 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:106(backward)\n",
      "      100    1.965    0.020    1.965    0.020 {method 'run_backward' of 'torch._C._EngineBase' objects}\n",
      "     4240    0.172    0.000    1.823    0.000 C:\\Users\\p\\AppData\\Local\\Temp\\ipykernel_33816\\2794650987.py:9(cpu_world_get_action)\n",
      "     4240    1.565    0.000    1.565    0.000 {built-in method torch.multinomial}\n",
      "     4240    0.005    0.000    1.043    0.000 C:\\Users\\p\\AppData\\Local\\Temp\\ipykernel_33816\\2794650987.py:3(gpu_world)\n",
      "25440/4240    0.060    0.000    1.038    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1494(_call_impl)\n",
      "     4240    0.011    0.000    1.017    0.000 C:\\Users\\p\\AppData\\Local\\Temp\\ipykernel_33816\\3306237402.py:11(forward)\n",
      "     4240    0.048    0.000    0.990    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215(forward)\n",
      "     8480    0.026    0.000    0.635    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:113(forward)\n",
      "     8480    0.600    0.000    0.600    0.000 {built-in method torch._C._nn.linear}\n",
      "     4240    0.010    0.000    0.358    0.000 C:\\Users\\p\\AppData\\Local\\Temp\\ipykernel_33816\\2794650987.py:6(cpu_world_simulate_env)\n",
      "     4448    0.309    0.000    0.309    0.000 {method 'to' of 'torch._C._TensorBase' objects}\n",
      "     5050    0.281    0.000    0.281    0.000 {method 'item' of 'torch._C._TensorBase' objects}\n",
      "      100    0.171    0.002    0.195    0.002 C:\\Users\\p\\AppData\\Local\\Temp\\ipykernel_33816\\2794650987.py:34(cpu_world_backward_pass)\n",
      "     4240    0.004    0.000    0.124    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:102(forward)\n",
      "     4240    0.005    0.000    0.119    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1446(relu)\n",
      "     4240    0.005    0.000    0.113    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1481(forward)\n",
      "     4240    0.113    0.000    0.113    0.000 {built-in method torch.relu}\n",
      "     4240    0.005    0.000    0.108    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1813(softmax)\n",
      "     4240    0.102    0.000    0.102    0.000 {method 'softmax' of 'torch._C._TensorBase' objects}\n",
      "     4240    0.007    0.000    0.090    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:46(step)\n",
      "     4240    0.084    0.000    0.084    0.000 {built-in method torch.log}\n",
      "     4240    0.003    0.000    0.083    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:52(step)\n",
      "     4240    0.004    0.000    0.080    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:45(step)\n",
      "     4344    0.078    0.000    0.078    0.000 {built-in method torch.tensor}\n",
      "     4240    0.036    0.000    0.076    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:132(step)\n",
      "      100    0.002    0.000    0.058    0.001 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:265(wrapper)\n",
      "      100    0.001    0.000    0.049    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:29(_use_grad)\n",
      "      100    0.001    0.000    0.048    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:108(step)\n",
      "      100    0.001    0.000    0.040    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:231(adam)\n",
      "      100    0.002    0.000    0.038    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:396(_multi_tensor_adam)\n",
      "    25540    0.035    0.000    0.035    0.000 {built-in method torch._C._get_tracing_state}\n",
      "     4240    0.022    0.000    0.023    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\gymnasium\\spaces\\discrete.py:94(contains)\n",
      "    21200    0.015    0.000    0.015    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1601(__getattr__)\n",
      "     4340    0.014    0.000    0.014    0.000 {built-in method numpy.array}\n",
      "      100    0.002    0.000    0.011    0.000 C:\\Users\\p\\AppData\\Local\\Temp\\ipykernel_33816\\226860480.py:1(discount_rewards)\n",
      "      100    0.002    0.000    0.008    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:435(zero_grad)\n",
      "      200    0.001    0.000    0.007    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\autograd\\profiler.py:491(__enter__)\n",
      "      200    0.000    0.000    0.006    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\_ops.py:497(__call__)\n",
      "      200    0.006    0.000    0.006    0.000 {built-in method torch._foreach_add_}\n",
      "     4240    0.004    0.000    0.006    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:207(__iter__)\n",
      "      100    0.000    0.000    0.006    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\_tensor.py:920(__iter__)\n",
      "      200    0.006    0.000    0.006    0.000 {built-in method torch._ops.profiler._record_function_enter_new}\n",
      "      100    0.005    0.000    0.005    0.000 {built-in method torch.stack}\n",
      "      200    0.005    0.000    0.005    0.000 {built-in method torch._foreach_mul_}\n",
      "      100    0.005    0.000    0.005    0.000 {method 'unbind' of 'torch._C._TensorBase' objects}\n",
      "      100    0.000    0.000    0.005    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:65(reset)\n",
      "      100    0.000    0.000    0.004    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:58(reset)\n",
      "      100    0.000    0.000    0.004    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:53(reset)\n",
      "      100    0.000    0.000    0.004    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:112(decorate_context)\n",
      "      100    0.001    0.000    0.004    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:193(reset)\n",
      "      100    0.004    0.000    0.004    0.000 {built-in method torch._foreach_add}\n",
      "      100    0.001    0.000    0.004    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:50(_make_grads)\n",
      "    21061    0.004    0.000    0.004    0.000 {method 'append' of 'list' objects}\n",
      "      100    0.004    0.000    0.004    0.000 {built-in method torch._foreach_sqrt}\n",
      "      200    0.001    0.000    0.004    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\autograd\\profiler.py:495(__exit__)\n",
      "      100    0.000    0.000    0.003    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:228(_cuda_graph_capture_health_check)\n",
      "      100    0.003    0.000    0.003    0.000 {built-in method torch.ones_like}\n",
      "      100    0.003    0.000    0.003    0.000 {method 'mean' of 'torch._C._TensorBase' objects}\n",
      "      100    0.002    0.000    0.003    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:66(_init_group)\n",
      "      100    0.002    0.000    0.003    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\utils\\_foreach_utils.py:20(_group_tensors_by_device_and_dtype)\n",
      "      100    0.003    0.000    0.003    0.000 {method 'std' of 'torch._C._TensorBase' objects}\n",
      "      100    0.003    0.000    0.003    0.000 {built-in method torch._foreach_addcmul_}\n",
      "     8580    0.003    0.000    0.003    0.000 {built-in method torch._C._has_torch_function_unary}\n",
      "      100    0.002    0.000    0.002    0.000 {built-in method torch._foreach_addcdiv_}\n",
      "      100    0.002    0.000    0.002    0.000 {method 'sum' of 'torch._C._TensorBase' objects}\n",
      "      100    0.002    0.000    0.002    0.000 {method 'uniform' of 'numpy.random._generator.Generator' objects}\n",
      "     4240    0.002    0.000    0.002    0.000 {built-in method math.cos}\n",
      "       10    0.000    0.000    0.002    0.000 {built-in method builtins.print}\n",
      "      100    0.002    0.000    0.002    0.000 {built-in method torch._foreach_div_}\n",
      "      100    0.000    0.000    0.002    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:94(is_available)\n",
      "       20    0.000    0.000    0.002    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\ipykernel\\iostream.py:610(write)\n",
      "      200    0.000    0.000    0.002    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\_ops.py:286(__call__)\n",
      "       20    0.000    0.000    0.002    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\ipykernel\\iostream.py:532(_schedule_flush)\n",
      "       10    0.000    0.000    0.002    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\ipykernel\\iostream.py:243(schedule)\n",
      "      800    0.001    0.000    0.002    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:39(_get_value)\n",
      "       10    0.002    0.000    0.002    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\zmq\\sugar\\socket.py:545(send)\n",
      "      100    0.001    0.000    0.002    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:489(<listcomp>)\n",
      "      200    0.002    0.000    0.002    0.000 {built-in method torch._ops.profiler.}\n",
      "      100    0.000    0.000    0.002    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:91(_nvml_based_avail)\n",
      "      200    0.001    0.000    0.002    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\autograd\\profiler.py:482(__init__)\n",
      "      100    0.000    0.000    0.001    0.000 <frozen os>:773(getenv)\n",
      "5517/5505    0.001    0.000    0.001    0.000 {built-in method builtins.isinstance}\n",
      "     4340    0.001    0.000    0.001    0.000 {built-in method builtins.iter}\n",
      "      100    0.000    0.000    0.001    0.000 <frozen _collections_abc>:771(get)\n",
      "     4240    0.001    0.000    0.001    0.000 {built-in method math.sin}\n",
      "      100    0.001    0.000    0.001    0.000 <frozen os>:674(__getitem__)\n",
      "      100    0.000    0.000    0.001    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:494(<listcomp>)\n",
      "      100    0.000    0.000    0.001    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:64(_default_to_fused_or_foreach)\n",
      "      100    0.000    0.000    0.001    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:490(<listcomp>)\n",
      "     4640    0.001    0.000    0.001    0.000 {method 'values' of 'collections.OrderedDict' objects}\n",
      "      300    0.000    0.000    0.001    0.000 {built-in method builtins.all}\n",
      "      416    0.001    0.000    0.001    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\autograd\\grad_mode.py:149(__init__)\n",
      "      400    0.000    0.000    0.001    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:52(_dispatch_sqrt)\n",
      "      100    0.000    0.000    0.001    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\cuda\\graphs.py:19(is_current_stream_capturing)\n",
      "        8    0.001    0.000    0.001    0.000 {built-in method torch.zeros_like}\n",
      "      100    0.001    0.000    0.001    0.000 {built-in method torch._C._cuda_isCurrentStreamCapturing}\n",
      "     1600    0.001    0.000    0.001    0.000 {built-in method torch.is_complex}\n",
      "      100    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:141(clone)\n",
      "      200    0.000    0.000    0.000    0.000 C:\\Python311\\Lib\\typing.py:338(inner)\n",
      "      100    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:433(<listcomp>)\n",
      "      100    0.000    0.000    0.000    0.000 <frozen os>:748(encodekey)\n",
      "      100    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\utils\\_foreach_utils.py:24(<listcomp>)\n",
      "      108    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\autograd\\grad_mode.py:48(__init__)\n",
      "      500    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:72(<genexpr>)\n",
      "      108    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\autograd\\grad_mode.py:53(__enter__)\n",
      "      100    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\utils\\_foreach_utils.py:27(<lambda>)\n",
      "      420    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\_tensor.py:942(__hash__)\n",
      "      100    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:434(<listcomp>)\n",
      "      100    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:435(<listcomp>)\n",
      "     1908    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\_jit_internal.py:1102(is_scripting)\n",
      "     2123    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "      108    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\autograd\\grad_mode.py:57(__exit__)\n",
      "      100    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:436(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1039(to)\n",
      "      100    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:87(_is_compiled)\n",
      "      500    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:268(<genexpr>)\n",
      "      624    0.000    0.000    0.000    0.000 {built-in method torch.is_grad_enabled}\n",
      "      6/1    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:795(_apply)\n",
      "      101    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}\n",
      "      100    0.000    0.000    0.000    0.000 <frozen os>:742(check_str)\n",
      "      100    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:492(<listcomp>)\n",
      "      100    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:46(_stack_if_compiling)\n",
      "     12/2    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2269(train)\n",
      "      100    0.000    0.000    0.000    0.000 {built-in method torch._C._are_functorch_transforms_active}\n",
      "      200    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "      400    0.000    0.000    0.000    0.000 {built-in method math.sqrt}\n",
      "      900    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\_utils.py:786(is_compiling)\n",
      "      416    0.000    0.000    0.000    0.000 {built-in method torch._C._set_grad_enabled}\n",
      "      100    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\utils\\_foreach_utils.py:27(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:14(__init__)\n",
      "      301    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:169(__init__)\n",
      "      100    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:98(_tensor_or_tensors_to_tuple)\n",
      "      420    0.000    0.000    0.000    0.000 {built-in method builtins.id}\n",
      "      100    0.000    0.000    0.000    0.000 {method 'numel' of 'torch._C._TensorBase' objects}\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2291(eval)\n",
      "       10    0.000    0.000    0.000    0.000 C:\\Python311\\Lib\\threading.py:1185(is_alive)\n",
      "      100    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\gymnasium\\core.py:215(np_random)\n",
      "      100    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C._TensorBase' objects}\n",
      "       12    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1617(__setattr__)\n",
      "      140    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "      100    0.000    0.000    0.000    0.000 {built-in method torch._C._cuda_getDeviceCount}\n",
      "       33    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2167(children)\n",
      "      100    0.000    0.000    0.000    0.000 {method 'upper' of 'str' objects}\n",
      "      200    0.000    0.000    0.000    0.000 {method '__exit__' of 'torch._C.DisableTorchFunctionSubclass' objects}\n",
      "       20    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\ipykernel\\iostream.py:505(_is_master_process)\n",
      "        8    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1139(convert)\n",
      "        5    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2059(parameters)\n",
      "       33    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2176(named_children)\n",
      "        5    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2084(named_parameters)\n",
      "        5    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2045(_named_members)\n",
      "      100    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\gymnasium\\core.py:121(reset)\n",
      "      200    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\jit\\__init__.py:89(annotate)\n",
      "       10    0.000    0.000    0.000    0.000 C:\\Python311\\Lib\\threading.py:1118(_wait_for_tstate_lock)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:489(add_param_group)\n",
      "      100    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\utils.py:17(maybe_parse_reset_bounds)\n",
      "       10    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\ipykernel\\iostream.py:127(_event_pipe)\n",
      "      100    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:249(_optimizer_step_code)\n",
      "      100    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
      "       12    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\parameter.py:8(__instancecheck__)\n",
      "        8    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:799(compute_should_use_set_data)\n",
      "     21/7    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2224(named_modules)\n",
      "       10    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method torch._C._nn._parse_to}\n",
      "       20    0.000    0.000    0.000    0.000 {built-in method nt.getpid}\n",
      "       20    0.000    0.000    0.000    0.000 {method 'write' of '_io.StringIO' objects}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'is_floating_point' of 'torch._C._TensorBase' objects}\n",
      "       20    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:291(_patch_step_function)\n",
      "       10    0.000    0.000    0.000    0.000 C:\\Python311\\Lib\\threading.py:568(is_set)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method torch._C._log_api_usage_once}\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method torch._has_compatible_shallow_copy_type}\n",
      "       25    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
      "       42    0.000    0.000    0.000    0.000 {method 'items' of 'collections.OrderedDict' objects}\n",
      "        6    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2113(<lambda>)\n",
      "       12    0.000    0.000    0.000    0.000 {function _ParameterMeta.__instancecheck__ at 0x0000024173B3B6A0}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'setdefault' of 'dict' objects}\n",
      "        8    0.000    0.000    0.000    0.000 c:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\torch\\__future__.py:20(get_overwrite_module_params_on_conversion)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'isdisjoint' of 'set' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "\n",
    "def profile_train(model):\n",
    "    pr = cProfile.Profile()\n",
    "    pr.enable()\n",
    "    train_for_profiling(model)\n",
    "    pr.disable()\n",
    "    s = io.StringIO()\n",
    "    ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')\n",
    "    ps.print_stats()\n",
    "    return s.getvalue()\n",
    "\n",
    "profile_output = profile_train(r_policy)\n",
    "print(profile_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE with TD Baseline aka. Actor Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce a `Value` (aka. `Critic`) net and rename the `Policy` into `Actor`. The `Critic` is meant to predict the expected score for a position, it is used to compute the advantage (give a baseline for how good was the episode). This is meant to help reduce the variance (converge faster).\n",
    "\n",
    "Advantage = discounted_reward - predicted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACActor(nn.Module):\n",
    "    def __init__(self, n_in, n_out, n_hidden=N_HIDDEN):\n",
    "        super().__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(n_in, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_out),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.f(x)\n",
    "    \n",
    "class ACCritic(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden=N_HIDDEN):\n",
    "        super().__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(n_in, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_actor = ACActor(n_in=IN, n_out=OUT).to(device)\n",
    "ac_critic = ACCritic(n_in=IN).to(device)\n",
    "ac_scores = []\n",
    "ac_validations = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     actor_loss:   3298.86 critic_loss:     17.03 explore: 49.47% len:  188 val_len:  521 score:     -6 val_score:   -412\n"
     ]
    }
   ],
   "source": [
    "def ac_train(actor, critic, epochs=10, lr=LR, gamma=GAMMA, max_steps=MAX_STEPS, log_every=100, scores=ac_scores, entropy_regularization=ENTROPY_REGULARIZATION):\n",
    "    actor = actor.to(device)\n",
    "    critic = critic.to(device)\n",
    "    actor.train()\n",
    "    critic.train()\n",
    "    opt_a = optim.Adam(actor.parameters(), lr=lr)\n",
    "    opt_c = optim.Adam(critic.parameters(), lr=lr)\n",
    "\n",
    "    pad = torch.zeros(1).to(device)\n",
    "    for epoch in range(epochs):\n",
    "        rewards, log_probs, values, entropies = [], [], [], []\n",
    "        state, _ = env.reset()\n",
    "        explored = 0\n",
    "        # forward\n",
    "        for t in range(max_steps):\n",
    "            state = torch.tensor(state).float().to(device)\n",
    "            values.append(critic(state))\n",
    "            actions = actor(state)\n",
    "            action = torch.multinomial(actions, 1)\n",
    "            explored += action != actions.argmax()\n",
    "            state, reward, done, _, _ = env.step(action.item())\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(torch.log(actions[action]))\n",
    "            entropies.append(-(actions * torch.log(actions)).sum()) # prey for no NaNs (otherwise add a + EPS)\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # backward critic\n",
    "        values = torch.cat(values)\n",
    "        next_values = torch.cat((values[1:].detach(), pad))\n",
    "        t_rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        target = t_rewards + gamma * next_values\n",
    "        critic_loss = F.mse_loss(values, target)\n",
    "        opt_c.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        opt_c.step()\n",
    "        # backward actor\n",
    "        losses = []\n",
    "        discounted_rewards = discount_rewards(rewards, gamma)\n",
    "        advantages = discounted_rewards - values.detach()\n",
    "        for log_prob, advantage in zip(log_probs, advantages):\n",
    "            losses.append(-log_prob * advantage)\n",
    "        entropy_loss = - torch.stack(entropies).sum() # negative because we want to increase entropy\n",
    "        actor_loss = torch.stack(losses).sum() + entropy_regularization * entropy_loss\n",
    "        opt_a.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        opt_a.step()\n",
    "\n",
    "        if epoch % log_every == 0:\n",
    "            val_len, val_score = validation(env, actor)\n",
    "            ac_validations.append(val_score)\n",
    "            print(f'{epoch:<5} actor_loss:{actor_loss.item():>10.2f} critic_loss:{critic_loss.item():>10.2f} explore:{100 * explored.item() / t:>6.2f}% len:{t:>5} val_len:{val_len:>5.0f} score:{sum(rewards):>7.0f} val_score:{val_score:>7.0f}')\n",
    "        scores.append(sum(rewards))\n",
    "    actor.eval()\n",
    "    critic.eval()\n",
    "\n",
    "ac_train(ac_actor, ac_critic, epochs=20000)\n",
    "plot_score(ac_scores, ac_validations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "# ----\n",
    "torch.save(ac_actor.state_dict(), 'weights/ac_actor-lunarlander.pt')\n",
    "torch.save(ac_critic.state_dict(), 'weights/ac_critic-lunarlander.pt')\n",
    "\n",
    "# load\n",
    "# ----\n",
    "# ac_actor = ACActor(n_in=IN, n_out=OUT).to(device)\n",
    "# ac_actor.load_state_dict(torch.load('weights/ac_actor-cartpole.pt'))\n",
    "# ac_critic = ACCritic(n_in=IN).to(device)\n",
    "# ac_critic.load_state_dict(torch.load('weights/ac_critic-cartpole.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 478 timesteps\n",
      "score=-902.77\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEWCAYAAACqitpwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAASV0lEQVR4nO3dW2zUZf7H8c8cOtMTBBoOFahQlEJtWQ/IOW1tEWyVoG406oUxGo0Xe6PxRl3jISbiRkVvXJQobiKgRoynjQaj2awiKFCgJWJbi93alp6mTKcH6LSd+f0vJq0g8LfAF2bavl/JZIaZHh4amHef+T2/Z1yO4zgCAMCQO94DAACMPcQFAGCOuAAAzBEXAIA54gIAMEdcAADmiAsAwBxxAQCYIy4AAHPekX6gy+W6mOMAAIwSI9nYhZkLAMAccQEAmCMuAABzxAUAYI64AADMERcAgDniAgAwR1wAAOaICwDAHHEBAJgjLgAAc8QFAGCOuAAAzBEXAIA54gIAMEdcAADmiAsAwBxxAQCYIy4AAHPEBQBgjrgAAMwRFwCAOeICADBHXAAA5ogLAMAccQEAmCMuAABzxAUAYI64AADMERcAgDniAgAwR1wAAOaICwDAHHEBAJgjLgAAc8QFAGCOuAAAzBEXAIA54gIAMEdcAADmiAsAwBxxAQCYIy4AAHPEBQBgjrgAAMwRFwCAOeICADBHXAAA5ogLAMAccQEAmCMuAABzxAUAYI64AADMERcAgDniAgAwR1wAAOaICwDAHHEBAJgjLgAAc8QFAGCOuAAAzBEXAIA54gIAMEdcAADmiAsAwBxxAQCYIy4AAHPEBQBgjrgAAMwRFwCAOeICADBHXAAA5ogLAMAccQEAmCMuAABzxAUAYI64AADMERcAgDniAgAwR1wAAOaICwDAHHEBAJgjLgAAc8QFAGCOuAAAzBEXAIA54gIAMEdcAADmiAsAwBxxAQCYIy4AAHPEBQBgjrgAAMwRFwCAOeICADBHXAAA5ogLAMAccQEAmCMuAABzxAUAYI64AADMERcAgDniAgAwR1wAAOaICwDAHHEBAJgjLgAAc8QFAGCOuAAAzBEXAIA54gIAMEdcAADmiAsAwBxxAQCYIy4AAHPEBQBgjrgAAMwRFwCAOeICADBHXAAA5ogLAMAccQEAmCMuAABzxAUAYI64AADMERcAgDniAgAwR1wAAOaICwDAHHEBAJgjLgAAc8QFAGCOuAAAzBEXAIA54gIAMEdcAADmiAsAwBxxAQCYIy4AAHPEBQBgjrgAAMwRFwCAOeICADBHXAAA5ogLAMAccQEAmCMuAABzxAUAYI64AADMERcAgDniAgAwR1wAAOaICwDAHHEBAJgjLgAAc8QFAGCOuAAAzHnjPQAAwOjg8/n0xBNPjOhjiQsA4E+53W6tX79ejzzyyIg+nrgg4TzzjDR7ttTfL33+uXTwYOz+nh4pFIrr0BLWnDnS009LjiN1dkr//KfU1ydFo1JHR+xnCZyvgoICPf300yoqKpLbPbKjKcQFCWfePGn+/NjtRYtiT5iOI/30k/Tdd7H729ulf/87fmNMNCkp0sKFv/+5sDD2MwuHpS++kFpapEhE2rtX+vnn+I0To09xcbG2b9+uyZMny+VyjfjziAsS0sn/hodu/+Uvvz+BhsPSvffGnkB7e6WNG2Ozmmg09kTa23vpxxxvf/x/73LFovPXv8b+7DixWUwoFLtdWSlt3x57rL9fqq+/tONFYsvIyNCyZcv0r3/9SxkZGef8+S7HcZwRfeA5FAu4EFu2SAsWnNvnDP0rHhiQ/vMf6dlnY7fHi9xc6d13z/3zhn5unZ3SP/4hff216bAwSmVkZGjLli1avXq1vN7zm4Mwc8GoNfTEODgoVVfHfvsOh6Wvvordh9Od/KtkMCj973+x+wIBaf/+uA0LCcLj8Sg/P18bNmxQcXHxBU0qiAsS2slPhtFo7LiB48Re+tq2LXZfOBybrZw4Eb9xJpqTf26RSOznNDgo/fe/0oEDsccbGqR9++I3RiQWt9ut+++/X88//7wyMzMv+OsRFyQkx4k9IdbWSq2tsdv79klffhl7PBKJrR7D74aCEg7HZiGDg7EVY1u3So2NscdOnGDlGE7n8Xj01FNP6fHHH5ff7zf5msQFCehv+vvfX1ckIh0+LDU3x3s8ic/vz9b33y/QF198qRMnpF27YgEG/syCBQv0yCOP6IEHHlBSUpLZ1yUuSEDL9dVXr8d7EKOKx5Oh+voF+uqrL+M9FIwiS5cu1datW5WdnT3i81dGasRx+eSTT9Ta2qrm5ubhS0tLi5qbm3X8+HFFIhENDg6ech2NRk0HCwC4cH6/X9ddd53ee+89zZ49+6J8jxHHZd26dZIkx3FOu4RCIbW1tam1tVXt7e1qaWlRe3u72tvb1dnZqWAweMp1Z2enBlnOAwCX3KRJk7R+/Xrde++9SktLu2jfZ8RxGVqSdqalaVOnTtXUqVOVl5d3yv2RSER9fX06fvy4Tpw4ccqlvb1dzc3NOnr0qFpaWtTU1KSWlhYFAgENDAyccunv79fg4KBGeEoOAOAPXC6Xpk+fro0bN2rt2rXnff7KSF3Ur+7xeJSWlnZaHc8WCcdx1N/fr0AgoEAgoLa2NgUCAbW3tysQCKijo0PHjh1TIBDQsWPH1NHRoY6ODvWOx9OxAeAc3HLLLXrllVc0b968S3JSfFwO6J/tL+ZyuZScnKxZs2Zp1qxZpzzmOI4GBgYUDodPuzQ3N+u7777T7t27deTIEQWDQYVCIV56AzDueTwePfTQQ3rmmWc0ffr0S7bbyqhZLeZyueTz+eTz+TRhwoRTHsvNzVVJSYkcx1FTU5N+/vlnHTp0SJWVlTpw4ICqqqrUz+J+AONMZmamHnzwQT377LPyeDyX9HuPmriMhMvlGp71rFq1Sn19ferp6VFHR4f27Nmj77//XpWVlcPHdng5DcBYNX/+fL3xxhtauXLlJQ+LNMbicjK3263U1FSlpqZq2rRpys3N1X333aeenh7V1NSotrZWFRUV2r9/v8rLyxUMBhWNRlk+DWBUc7vdKisr04svvqi8vLy4bTo8ZuNyNunp6bruuut07bXX6vbbb1c4HFZvb6+qqqq0c+dO7dmzR7/99puamprU0dFBbACMGunp6Xr44Yf15JNPntc2+ZbGXVyGuFwuJSUlKSkpSenp6Zo+fbqKioo0ODioxsZG1dXVqaamRj/++KP27t2rX3/9VQMDAyyJBpCQkpOT9dJLL13081dGatzG5Wy8Xq/mzJmjOXPmqKioSPfff78GBgbU1NSkffv2affu3aqqqlJdXZ0aGhpYKAAg7pYsWaKXXnopbsdXzoS4/D/cbvfwCrWcnBzl5OTonnvuUTAYVGNjoxoaGlReXq5du3bp0KFD6urqUjgc1sB4epcqAHHjdrtVWlqqF198Ufn5+Qn1po7E5Ry5XC5lZGQoIyNDCxcuVFlZmSKRiHp6enTw4EGVl5frwIEDqq2tVXV1tUKhULyHDGAMmjhxou655x69+uqrSk5OTqiwSMTlgrhcLrlcLrndbk2ePFnFxcW64YYb1NfXp0AgoNbW1tN2Ghi6PXTd19enwcHBUy5Dx3Yi7JkO4AymT5+uF154QXfffbdSUlLiPZwzIi7GXC6XUlJSlJWVpaysrDMe/B+6z3EcdXd3KxgM6tixYwoGg6fdDoVC6uzsHL7u6upSMBhUV1cX5+kA49CiRYv08ssvq7Cw0HybfEvE5SI701T15PsmTZqkSZMmKTs7+7SPO3nLmzNd9/b2Ds+ChnahHpoRtba2qqenR/39/cObf558GRgYYNUbMIr4/X6VlZVpw4YNmjNnTsK9DPZHxCWBnbzlzZn8WRzC4fDwWxycPPsZutTU1Oinn37S4cOH1d3dfTH+CgAMuN1uPfbYY3r00Uc1ZcqUeA9nRIjLKPZnv7kkJycrMzNTmZmZpz3mOM7w9jihUEhVVVXavXu3ysvL1dDQMLzzNJt/AvGVnZ2t5557TnfccUfCHl85E+IyTg0dG0pJSdHUqVN15ZVXau3atYpEIjp69Khqa2tVW1uryspKVVRU6NChQ+ru7lY0GuXlNOASufrqq7VhwwaVlJTEeyjnjLjgFB6PZ3gxwg033KD+/n6Fw2F1d3erurpaP/zwg8rLy1VfX6+Wlha1tbVxXg9gyO12Kzs7W4sXL9b69et1+eWXx3tI54W44KxcLpf8fr/8fr8mTpyomTNnqqSkRNFoVM3Nzaqvr1ddXZ0qKyu1f/9+VVRUqKuri2XUwDnyer1KSUnRqlWrVFpaqqVLl+qqq6466/HW0YC44Jy53W7NnDlTM2fO1PLly3XXXXdpYGBAx48fV3V1tfbs2aODBw/qyJEjamxsVFNTE7Mb4Azmzp2r/Px83XLLLSotLdW0adPk8/kSeonxSBEXXBCXyyWv1zv8m9eKFSu0YsUKRaNRtbW1qbm5Wb/99psqKiq0d+9eHTx4UKFQaHg5NcdvMJ74fD6lpqZq5cqVuu2223T99dcrJydHqamp8R6aOeKCi8Ltdg+vVLvmmmu0du1aRaNRhcNhVVdXq7y8XJWVlaqpqVFdXZ3q6+uZ3WDMysrK0uLFi1VSUqK1a9dqxowZ8nq9CX+uyoVwOfzqiDiJRCIKBoNqb2/X0aNHVVFRoR9++EF9fX2qqKiI9/BGlfT0dPn9flVUVPAeRAnA5/Np0qRJuuaaa3TXXXdp8eLFys7OVnp6eryHdskQFySEk7fEOfkaI/fLL79o06ZN2r59uxoaGuI9nHFp2rRpKiwsVGFhocrKyjR37tzhPQjHG+ICjCH9/f2qq6vT5s2b9dFHH6m+vp4TYS8in8+nqVOnKi8vT3feeacKCgp02WWXaeLEifEeWtwRF2AMchxHR44c0bvvvqtt27aptrY23kMaUyZPnqyioiKtXr1ahYWFysvLk/Tnu2aMJ8QFGMOG3kV127Ztev/991VdXc27p54Hr9erGTNmKDc3V7feeqtuuukmTZkyRRMmTCAoZ0FcgHGirq5OH3/8sd555x0dPnyYA/9/wu12Ky0tTcXFxbrpppu0bNkyXX311QnzNsKJjrgA40gkElFbW5s+++wzbd68WYcOHdKJEyfiPayE4Xa7NXv2bM2fP1/r1q1TWVmZpkyZorS0NGYo54i4AOOQ4zhqamrSjh07tHHjRlVWVo7b84y8Xq/8fr9KSkpUWlqqZcuWKS8vT36/P95DG9WICzCORaNRhUIh7dixQ2+++aZ+/PHHcTGT8fv9ys7O1rx581RaWqqbb75ZmZmZY2brlURAXADIcRy1t7fr22+/1Wuvvab9+/ePqcgkJSUpOTlZWVlZWr16tQoKCpSbm6vs7OxR9R4powlxATDMcRz19/drx44devvtt/XNN9+ot7c33sM6Lx6PRzk5OcrLy9OKFStUVFSkvLw8eb1eud1ujqFcZMQFwBl1dnZq9+7dev3117Vz5051dXUl9M4JSUlJmjBhgmbNmqUbb7xRxcXFysnJ0YwZMzggHwfEBcBZOY6jaDSqr7/+Wlu2bNFnn32mrq6ueA9rmMfj0RVXXKFFixZpyZIlKigoUF5ennw+37jddiVREBcAI9Ld3a0DBw7orbfe0o4dOxQIBC75uTI+n08ZGRmaMWOGVq1apVWrVmnevHmaNm3auNoUcjQgLgBGbGgms2vXLm3dulUffvihjh07dlG/p8fj0eWXX66VK1dqyZIlWr58ufLz84eXCjM7SUzEBcB5OX78uKqqqvT222/r888/V1NTk8lMZmgzyFmzZqmwsFBr1qzRggULNHnyZKWmphKTUYK4ALggg4ODqqys1NatW7Vt2za1trae04H/oWMjWVlZKioq0rJly7Ro0SLl5+crJSWFmIxSxAXABRtawtzY2KhNmzbp008/VU1NzVkjM7QR5Ny5c7V06VKVlZUpNzdX6enpSk5O5kTGMYC4ADAViURUXV2tDz74QJs3b9bRo0cl/R6UgoICFRUVKT8/X7m5ubz3yRhFXACYcxxHkUhEHR0d2rRpk/r6+rRmzRotXLhQaWlpw0uFMXYRFwCAOV7YBACYIy4AAHPEBQBgjrgAAMwRFwCAOeICADBHXAAA5ogLAMAccQEAmCMuAABzxAUAYI64AADMERcAgDniAgAwR1wAAOaICwDAHHEBAJgjLgAAc8QFAGCOuAAAzBEXAIA54gIAMEdcAADmiAsAwBxxAQCYIy4AAHPEBQBgjrgAAMwRFwCAOeICADBHXAAA5ogLAMAccQEAmCMuAABzxAUAYI64AADMERcAgDniAgAwR1wAAOaICwDAHHEBAJgjLgAAc8QFAGCOuAAAzBEXAIA54gIAMEdcAADmiAsAwBxxAQCYIy4AAHPEBQBgjrgAAMwRFwCAOeICADD3f9g6aKE148mQAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_play(env, ac_actor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C (Advantage Actor Critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, n_in, n_out, n_hidden=10):\n",
    "        super().__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(n_in, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_out),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.f(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden=10):\n",
    "        super().__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(n_in, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.f(x)\n",
    "\n",
    "actor = Actor(n_in=10, n_out=2)\n",
    "critic = Critic(n_in=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=100, lr=LR):\n",
    "    actor_opt = optim.Adam(actor.parameters(), lr=lr)\n",
    "    critic_opt = optim.Adam(critic.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action_probs = actor(torch.from_numpy(state).float())\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            V_current = critic(torch.from_numpy(state).float())\n",
    "            V_next = critic(torch.from_numpy(next_state).float())\n",
    "\n",
    "            # Compute advantage and TD-target\n",
    "            advantage = reward + (1 - int(done)) * V_next - V_current\n",
    "            td_target = reward + (1 - int(done)) * V_next\n",
    "\n",
    "            # Update the critic\n",
    "            critic_loss = advantage.pow(2)\n",
    "            critic_opt.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_opt.step()\n",
    "\n",
    "            # Update the actor\n",
    "            actor_loss = -torch.log(action_probs[action]) * advantage.detach()\n",
    "            actor_opt.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_opt.step()\n",
    "\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L^{CPI}(\\theta) = \\mathbb{E}_t [ \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{{\\theta}_{old}(a_t | s_t)}} A_t] = \\mathbb{E}_t [r_t(\\theta)A_t]$\n",
    "\n",
    "$L^{CLIP}(\\theta) = \\mathbb{E}_t [min(r_t(\\theta)A_t, clip(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) A_t)]$\n",
    "\n",
    "\n",
    "- $\\mathbb{E}$ : expected value\n",
    "- $A$: advantage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_hf_nlp",
   "language": "python",
   "name": "venv_hf_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
